{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "py.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 7\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/diabetes.csv')\n",
    "df_name = df.columns\n",
    "df_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.pairplot(df, hue = \"Outcome\", palette = 'husl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotHist(df,nameOfFeature):\n",
    "    cls_train = df[nameOfFeature]\n",
    "    data_array = cls_train\n",
    "    hist_data = np.histogram(data_array)\n",
    "    binsize = .5\n",
    "\n",
    "    trace1 = go.Histogram(\n",
    "        x=data_array,\n",
    "        autobinx=False,\n",
    "        xbins=dict(\n",
    "            start=df[nameOfFeature].min()-1,\n",
    "            end=df[nameOfFeature].max()+1,\n",
    "            size=binsize\n",
    "        )\n",
    "    )\n",
    "\n",
    "    trace_data = [trace1]\n",
    "    layout = go.Layout(\n",
    "        title='The distribution of ' + nameOfFeature,\n",
    "        xaxis=dict(\n",
    "            title=nameOfFeature,\n",
    "            titlefont=dict(\n",
    "                family='Courier New, monospace',\n",
    "                size=18,\n",
    "                color='#7f7f7f'\n",
    "            )\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title='Number of labels',\n",
    "            titlefont=dict(\n",
    "                family='Courier New, monospace',\n",
    "                size=18,\n",
    "                color='#7f7f7f'\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    fig = go.Figure(data=trace_data, layout=layout)\n",
    "    py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHist(df,'Pregnancies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew\n",
    "from scipy.stats import kurtosis\n",
    "def plotBarCat(df,feature,target):\n",
    "    \n",
    "    \n",
    "    \n",
    "    x0 = df[df[target]==0][feature] #x0 contains values only for outcome 0\n",
    "    x1 = df[df[target]==1][feature] #x1 contains values only for outcome 1\n",
    "\n",
    "    trace1 = go.Histogram(\n",
    "        x=x0,\n",
    "        opacity=0.75\n",
    "    )\n",
    "    trace2 = go.Histogram(\n",
    "        x=x1,\n",
    "        opacity=0.75\n",
    "    )\n",
    "\n",
    "    data = [trace1, trace2]\n",
    "    layout = go.Layout(barmode='overlay',\n",
    "                      title=feature,\n",
    "                       yaxis=dict(title='Count'\n",
    "        ))\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "    py.iplot(fig, filename='overlaid histogram')\n",
    "    \n",
    "    def DescribeFloatSkewKurt(df,target):\n",
    "        \"\"\"\n",
    "            A fundamental task in many statistical analyses is to characterize\n",
    "            the location and variability of a data set. A further\n",
    "            characterization of the data includes skewness and kurtosis.\n",
    "            Skewness is a measure of symmetry, or more precisely, the lack\n",
    "            of symmetry. A distribution, or data set, is symmetric if it\n",
    "            looks the same to the left and right of the center point.\n",
    "            Kurtosis is a measure of whether the data are heavy-tailed\n",
    "            or light-tailed relative to a normal distribution. That is,\n",
    "            data sets with high kurtosis tend to have heavy tails, or\n",
    "            outliers. Data sets with low kurtosis tend to have light\n",
    "            tails, or lack of outliers. A uniform distribution would\n",
    "            be the extreme case\n",
    "        \"\"\"\n",
    "        print('-*-'*25)\n",
    "        print(\"{0} mean : \".format(target), np.mean(df[target]))\n",
    "        print(\"{0} var  : \".format(target), np.var(df[target]))\n",
    "        print(\"{0} skew : \".format(target), skew(df[target]))\n",
    "        print(\"{0} kurt : \".format(target), kurtosis(df[target]))\n",
    "        print('-*-'*25)\n",
    "    \n",
    "    DescribeFloatSkewKurt(df,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotBarCat(df,df_name[0],'Outcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotBarCat(df,df_name[1],'Outcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotBarCat(df,df_name[2],'Outcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotBarCat(df,df_name[3],'Outcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotBarCat(df,df_name[4],'Outcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotBarCat(df,df_name[5],'Outcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotBarCat(df,df_name[6],'Outcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotBarCat(df,df_name[7],'Outcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotBarCat(df,df_name[8],'Outcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotPie(df, nameOfFeature):\n",
    "    labels = [str(df[nameOfFeature].unique()[i]) for i in range(df[nameOfFeature].nunique())]\n",
    "    values = [df[nameOfFeature].value_counts()[i] for i in range(df[nameOfFeature].nunique())]\n",
    "\n",
    "    trace=go.Pie(labels=labels,values=values)\n",
    "\n",
    "    py.iplot([trace])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotPie(df, 'Outcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OutLiersBox(df, nameOfFeature):\n",
    "    trace0 = go.Box(\n",
    "                y = df[nameOfFeature],\n",
    "                name = \"All Points\",\n",
    "                jitter = 0.3,\n",
    "                pointpos = -1.8,\n",
    "                boxpoints = 'all',\n",
    "                marker = dict(\n",
    "                    color = 'rgb(7,40,89)'),\n",
    "                line = dict(\n",
    "                    color = 'rgb(7,40,89)')\n",
    "    )\n",
    "    \n",
    "    trace1 = go.Box(\n",
    "                y = df[nameOfFeature],\n",
    "                name = \"Only Whiskers\",\n",
    "                boxpoints = False,\n",
    "                marker = dict(\n",
    "                    color = 'rgb(9,56,125)'),\n",
    "                line = dict(\n",
    "                    color = 'rgb(9,56,125)')\n",
    "    )\n",
    "    \n",
    "    trace2 = go.Box(\n",
    "                y = df[nameOfFeature],\n",
    "                name = \"Suspected Outliers\",\n",
    "                boxpoints = \"suspectedoutliers\",\n",
    "                marker = dict(\n",
    "                    color = 'rgb(8,81,156)',\n",
    "                    outliercolor = 'rgba(219, 64, 82, 0.6)',\n",
    "                    line = dict(\n",
    "                        outliercolor = 'rgba(219, 64, 82, 0.6)',\n",
    "                        outlierwidth = 2)),\n",
    "                line = dict(\n",
    "                    color = 'rgb(8,81,156)')\n",
    "    )\n",
    "    \n",
    "    trace3 = go.Box(\n",
    "                y = df[nameOfFeature],\n",
    "                name = \"Whiskers and Outliers\",\n",
    "                boxpoints = 'outliers',\n",
    "                marker = dict(\n",
    "                    color = 'rgb(107,174,214)'),\n",
    "                line = dict(\n",
    "                    color = 'rgb(107,174,214)')\n",
    "    )\n",
    "    \n",
    "    data = [trace0, trace1, trace2, trace3]\n",
    "    layout = go.Layout(\n",
    "            title = \"{} Outliers\".format(nameOfFeature)\n",
    "    )\n",
    "    \n",
    "    fig = go.Figure(data = data, layout = layout)\n",
    "    py.iplot(fig, filename = \"Outliers\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OutLiersBox(df, df_name[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OutLiersBox(df, df_name[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OutLiersBox(df, df_name[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OutLiersBox(df, df_name[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OutLiersBox(df, df_name[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OutLiersBox(df, df_name[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OutLiersBox(df, df_name[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OutLiersBox(df, df_name[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers Investigation Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.ensemble import  IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OutlierDetection(df, feature1, feature2, outliers_fraction = .1):\n",
    "    new_df = df.copy()\n",
    "    rng = np.random.RandomState(42)\n",
    "    \n",
    "    n_samples = new_df.shape[0]\n",
    "    clusters_separation = [0]\n",
    "    \n",
    "    classifiers = {\n",
    "        \"One-Class SVM\" : svm.OneClassSVM(nu = 0.95*outliers_fraction+0.05,\n",
    "                                         kernel = \"rbf\", gamma = 0.1),\n",
    "        \"Robust Covariance\" : EllipticEnvelope(contamination = outliers_fraction),\n",
    "        \"Isolation Forest\" : IsolationForest(max_samples = n_samples, \n",
    "                                            contamination = outliers_fraction, \n",
    "                                            random_state = rng),\n",
    "        \"Local Outlier Factor\" : LocalOutlierFactor(\n",
    "                                                    n_neighbors = 35,\n",
    "                                                    contamination = outliers_fraction)\n",
    "    }\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.linspace(new_df[feature1].min()-new_df[feature1].min()*10/100,\n",
    "                                    new_df[feature1].max()-new_df[feature1].max()*10/100, 50),\n",
    "                        np.linspace(new_df[feature2].min()-new_df[feature2].min()*10/100,\n",
    "                                   new_df[feature2].max()-new_df[feature2].max()*10/100, 50))\n",
    "    n_inliers = int((1. - outliers_fraction)*n_samples)\n",
    "    n_outliers = int(outliers_fraction*n_samples)\n",
    "    ground_truth = np.ones(n_samples, dtype = int)\n",
    "    ground_truth[-n_outliers:] = -1\n",
    "    \n",
    "    for i, offset in enumerate(clusters_separation):\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        X = new_df[[feature1, feature2]].values.tolist()\n",
    "        \n",
    "        plt.figure(figsize=(9,7))\n",
    "        for i, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "            if clf_name == \"Local Outlier Factor\":\n",
    "                y_pred = clf.fit_predict(X)\n",
    "                scores_pred = clf.negative_outlier_factor_\n",
    "            else:\n",
    "                clf.fit(X)\n",
    "                scores_pred = clf.decision_function(X)\n",
    "                y_pred = clf.predict(X)\n",
    "            \n",
    "            threshold = stats.scoreatpercentile(scores_pred, 100*outliers_fraction)\n",
    "            n_errors = (y_pred != ground_truth).sum()\n",
    "            \n",
    "            unique, counts = np.unique(y_pred, return_counts = True)\n",
    "            print(clf_name, dict(zip(unique, counts)))\n",
    "            \n",
    "            new_df[feature1+\"_\"+feature2+clf_name] = y_pred\n",
    "            \n",
    "            if clf_name == \"Local Outlier Factor\":\n",
    "                Z = clf._decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "            else:\n",
    "                Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "            \n",
    "            Z = Z.reshape(xx.shape)\n",
    "            subplot = plt.subplot(2, 2, i+1)\n",
    "            subplot.contourf(xx, yy, Z, levels = np.linspace(Z.min(), threshold, 7),\n",
    "                            cmap = plt.cm.Blues_r)\n",
    "            \n",
    "            a = subplot.contour(xx, yy, Z, levels = [threshold],\n",
    "                               linewidths =2, colors = 'red')\n",
    "            subplot.contourf(xx, yy, Z, levels = [threshold, Z.max()],\n",
    "                            colors = 'orange')\n",
    "            b = plt.scatter(new_df[feature1], new_df[feature2], c = \"white\",\n",
    "                           s = 20, edgecolor = 'k')\n",
    "            \n",
    "            subplot.axis('tight')\n",
    "            subplot.set_xlabel(\"%s\"%(feature1))\n",
    "            \n",
    "            plt.ylabel(feature2)\n",
    "            plt.title(\"%d %s (errors: %d)\"%(i+1, clf_name, n_errors))\n",
    "            \n",
    "        plt.subplots_adjust(0.04, 0.1, 0.96, 0.94, 0.1, 0.26)\n",
    "        \n",
    "    plt.show()\n",
    "    return new_df\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = OutlierDetection(df, \"Pregnancies\", \"BloodPressure\",.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import set_option\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[df_name[0:8]]\n",
    "Y = df[df_name[8]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.25, random_state = 0, stratify = df['Outcome'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spot Check Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetBasedModel():\n",
    "    basedModels = []\n",
    "    basedModels.append((\"LR\", LogisticRegression()))\n",
    "    basedModels.append((\"LDA\", LinearDiscriminantAnalysis()))\n",
    "    basedModels.append((\"KNN\", KNeighborsClassifier()))\n",
    "    basedModels.append((\"CART\", DecisionTreeClassifier()))\n",
    "    basedModels.append((\"NB\", GaussianNB()))\n",
    "    basedModels.append((\"SVM\", SVC(probability = True)))\n",
    "    basedModels.append((\"AB\", AdaBoostClassifier()))\n",
    "    basedModels.append((\"GBM\", GradientBoostingClassifier()))\n",
    "    basedModels.append((\"RF\", RandomForestClassifier()))\n",
    "    basedModels.append((\"ET\", ExtraTreesClassifier()))\n",
    "    \n",
    "    \n",
    "    return basedModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BasedLine2(X_train, y_train, models):\n",
    "    num_folds = 10\n",
    "    scoring = 'accuracy'\n",
    "    \n",
    "    results = []\n",
    "    names = []\n",
    "    \n",
    "    for name, model in models:\n",
    "        kfold = StratifiedKFold(n_splits = num_folds, random_state = SEED)\n",
    "        cv_results = cross_val_score(model, X_train, y_train, cv = kfold, scoring = scoring)\n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "        msg = \"%s : %f (%f)\" %(name, cv_results.mean(),cv_results.std())\n",
    "        print(msg)\n",
    "        \n",
    "    return names, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotBoxR(object):\n",
    "    \n",
    "    def __Trace(self, nameOfFeature, value):\n",
    "        \n",
    "        trace = go.Box(\n",
    "            y = value,\n",
    "            name = nameOfFeature, \n",
    "            marker = dict(\n",
    "                color = 'rgb(0,128,128)',\n",
    "            )\n",
    "        )\n",
    "        return trace\n",
    "    def PlotResult(self, names, results):\n",
    "        data = []\n",
    "        \n",
    "        for i in range(len(names)):\n",
    "            data.append(self.__Trace(names[i], results[i]))\n",
    "            \n",
    "        py.iplot(data)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = GetBasedModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names, results = BasedLine2(X_train, y_train, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotBoxR().PlotResult(names, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScoreDataFrame(names, results):\n",
    "    def floatingDecimals(f_val, dec = 3):\n",
    "        prc = \"{:.\" +str(dec)+ \"f}\"\n",
    "        return float(prc.format(f_val))\n",
    "    \n",
    "    scores = []\n",
    "    for r in results:\n",
    "        scores.append(floatingDecimals(r.mean(), 4))\n",
    "    \n",
    "    scoreDataFrame = pd.DataFrame({'Model':names, 'Score':scores})\n",
    "    return scoreDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedLineScore = ScoreDataFrame(names, results)\n",
    "basedLineScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing - Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def GetScaledModel(nameOfScaler):\n",
    "    \n",
    "    if nameOfScaler == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif nameOfScaler == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "        \n",
    "    pipelines = []\n",
    "    \n",
    "    pipelines.append((nameOfScaler+'LR', Pipeline([('Scaler', scaler), ('LR', LogisticRegression())]) ))\n",
    "    pipelines.append((nameOfScaler+'LDA', Pipeline([('Scaler', scaler), ('LDA', LinearDiscriminantAnalysis())]) ))\n",
    "    pipelines.append((nameOfScaler+'KNN', Pipeline([('Scaler', scaler), ('KNN', KNeighborsClassifier())]) ))\n",
    "    pipelines.append((nameOfScaler+'CART', Pipeline([('Scaler', scaler), ('CART', DecisionTreeClassifier())]) ))\n",
    "    pipelines.append((nameOfScaler+'NB', Pipeline([('Scaler', scaler), ('LR', GaussianNB())]) ))\n",
    "    pipelines.append((nameOfScaler+'SVM', Pipeline([('Scaler', scaler), ('SVM', SVC())]) ))\n",
    "    pipelines.append((nameOfScaler+'AB', Pipeline([('Scaler', scaler), ('AB', AdaBoostClassifier())]) ))\n",
    "    pipelines.append((nameOfScaler+'GMB', Pipeline([('Scaler', scaler), ('GMB', GradientBoostingClassifier())]) ))\n",
    "    pipelines.append((nameOfScaler+'RF', Pipeline([('Scaler', scaler), ('Rf', RandomForestClassifier())]) ))\n",
    "    pipelines.append((nameOfScaler+'ET', Pipeline([('Scaler', scaler), ('ET', ExtraTreesClassifier())]) ))\n",
    "    \n",
    "    return pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StandardScaler - Model Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = GetScaledModel('standard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names, results = BasedLine2(X_train, y_train, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotBoxR().PlotResult(names, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaledScoreStandard = ScoreDataFrame(names, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing BasedModels Score with StandardScaled Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compareModels = pd.concat([basedLineScore, scaledScoreStandard], axis = 1)\n",
    "compareModels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinMaxScaler - Model Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = GetScaledModel('minmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names, results = BasedLine2(X_train, y_train, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotBoxR().PlotResult(names, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaledScoreMinMax = ScoreDataFrame(names, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compared BasedLine models score with MinMax scaled models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compareModels = pd.concat([basedLineScore, scaledScoreMinMax], axis = 1)\n",
    "compareModels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of all the above models i.e., basedLine, StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compareModels = pd.concat([basedLineScore, scaledScoreStandard, scaledScoreMinMax], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compareModels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Outliers/Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t_name = df_t.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TurkyOutliers(df_out, nameOfFeature, drop = False):\n",
    "    valueOfFeature = df_out[nameOfFeature]\n",
    "    \n",
    "    Q1 = np.percentile(valueOfFeature, 25.)\n",
    "    \n",
    "    Q3 = np.percentile(valueOfFeature, 75.)\n",
    "    \n",
    "    step = (Q3-Q1)*1.5\n",
    "    \n",
    "    outliers = valueOfFeature[~((valueOfFeature >= Q1-step) & (valueOfFeature <= Q3 + step))].index.tolist()\n",
    "    feature_outliers = valueOfFeature[~((valueOfFeature >= Q1-step)&(valueOfFeature <= Q3+step))].values\n",
    "    \n",
    "    print(\"Number of outliers (inc duplicates): {} and outliers: {}\".format(len(outliers), feature_outliers))\n",
    "    \n",
    "    if drop:\n",
    "        good_data = df_out.drop(df_out.index[outliers]).reset_index(drop = True)\n",
    "        print(\"New dataset with removed outliers has {} samples with {} features each \".format(*good_data.shape))\n",
    "        return good_data\n",
    "    else:\n",
    "        print(\"Nothing happens, df.shape = \",df_out.shape)\n",
    "        return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers Detected vs After Cleaning Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_number = 0\n",
    "OutLiersBox(df, df_name[feature_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = TurkyOutliers(df_t, df_name[feature_number], True)\n",
    "OutLiersBox(df_clean, df_name[feature_number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_number = 1\n",
    "OutLiersBox(df, df_name[feature_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = TurkyOutliers(df_clean, df_name[feature_number], True)\n",
    "OutLiersBox(df_clean, df_name[feature_number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_number = 2\n",
    "OutLiersBox(df, df_name[feature_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = TurkyOutliers(df_clean, df_name[feature_number], True)\n",
    "OutLiersBox(df_clean, df_name[feature_number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_number = 3\n",
    "OutLiersBox(df, df_name[feature_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = TurkyOutliers(df_clean, df_name[feature_number], True)\n",
    "OutLiersBox(df_clean, df_name[feature_number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_number = 4\n",
    "OutLiersBox(df, df_name[feature_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = TurkyOutliers(df_clean, df_name[feature_number], True)\n",
    "OutLiersBox(df_clean, df_name[feature_number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_number = 5\n",
    "OutLiersBox(df, df_name[feature_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = TurkyOutliers(df_clean, df_name[feature_number], True)\n",
    "OutLiersBox(df_clean, df_name[feature_number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_number = 6\n",
    "OutLiersBox(df, df_name[feature_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = TurkyOutliers(df_clean, df_name[feature_number], True)\n",
    "OutLiersBox(df_clean, df_name[feature_number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_number = 7\n",
    "OutLiersBox(df, df_name[feature_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = TurkyOutliers(df_clean, df_name[feature_number], True)\n",
    "OutLiersBox(df_clean, df_name[feature_number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_number = 8\n",
    "OutLiersBox(df, df_name[feature_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = TurkyOutliers(df_clean, df_name[feature_number], True)\n",
    "OutLiersBox(df_clean, df_name[feature_number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('df shape: {}, new df shape: {}, we lost {} rows, {}% of our data'.format(df.shape[0],df_clean.shape[0],\n",
    "                                                              df.shape[0]-df_clean.shape[0],\n",
    "                                                        (df.shape[0]-df_clean.shape[0])/df.shape[0]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detection Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = OutlierDetection(df, 'Pregnancies', 'BloodPressure', .1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detection Plots after data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_t = OutlierDetection(df_clean, 'Pregnancies', 'BloodPressure', .1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing accuracy of models after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_name = df_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_c = df_clean[df_clean_name[0:8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_c = df_clean[df_clean_name[8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X_c, Y_c, test_size = 0.25,\n",
    "                                                            random_state = 0, stratify = df_clean['Outcome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = GetScaledModel('minmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names, results = BasedLine2(X_train_c, y_train_c, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotBoxR().PlotResult(names, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaledScoreMinMax_c = ScoreDataFrame(names, results)\n",
    "compareModels = pd.concat([basedLineScore, scaledScoreStandard, \n",
    "                           scaledScoreMinMax,scaledScoreMinMax_c], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compareModels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High correlation leads to over-fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HeatMap(df, x = True):\n",
    "    correlations = df.corr()\n",
    "    \n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap = True)\n",
    "    fig, ax = plt.subplots(figsize = (10, 10))\n",
    "    fig = sns.heatmap(correlations, cmap = cmap, vmax = 1.0, center = 0,\n",
    "                      fmt = '.2f', square = True, linewidths = .5, annot = x, cbar_kws = {\"shrink\" : .75})\n",
    "    fig.set_xticklabels(fig.get_xticklabels(), rotation = 90, fontsize = 10)\n",
    "    fig.set_yticklabels(fig.get_xticklabels(), rotation = 0, fontsize = 10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "HeatMap(df, x = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using EXTRA TREE CLASSIFIER\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators = 250, random_state = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train_c, y_train_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = 100.0 * (feature_importance/feature_importance.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_idx = np.argsort(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = np.arange(sorted_idx.shape[0]) + .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,2)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align = 'center')\n",
    "plt.yticks(pos, df.columns[sorted_idx])\n",
    "plt.xlabel(\"Relative Importance\")\n",
    "plt.ylabel(\"Variable Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Accuracy with the most important features (Top 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_imp = df_clean[['Glucose', 'BMI', 'Age', 'DiabetesPedigreeFunction', 'Outcome']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_imp_name = df_feature_imp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_feature_imp[df_feature_imp_name[0 : df_feature_imp.shape[1]-1]]\n",
    "Y = df_feature_imp[df_feature_imp_name[df_feature_imp.shape[1]-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_im, X_test_im, y_train_im, y_test_im = train_test_split(X, Y, test_size = 0.1,\n",
    "                                                                random_state = 0, stratify = df_feature_imp['Outcome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = GetScaledModel('minmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names, results = BasedLine2(X_train_im, y_train_im, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotBoxR().PlotResult(names, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaledScoreMinMax_im = ScoreDataFrame(names, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compareModels = pd.concat([basedLineScore, scaledScoreStandard, scaledScoreMinMax, scaledScoreMinMax_c,\n",
    "                          scaledScoreMinMax_im], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compareModels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We still could improve predictions..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unscaled = df_clean[['Glucose', 'BMI', 'Age', 'DiabetesPedigreeFunction', 'Outcome']]\n",
    "df_imp_scaled_name = df_unscaled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imp_scaled = MinMaxScaler().fit_transform(df_unscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_imp_scaled[:, 0:4]\n",
    "Y = df_imp_scaled[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sc, X_test_sc, y_train_sc, y_test_sc = train_test_split(X, Y, test_size = 0.1,\n",
    "                                                               random_state = 0, stratify = df_imp_scaled[:,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomSearch & Grid Search Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from scipy.stats import uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomSearch Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomSearch(object):\n",
    "    def __init__(self, X_train, y_train, model,hyperparameters):\n",
    "        \n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.model = model\n",
    "        self.hyperparameters = hyperparameters\n",
    "    \n",
    "    def RandomSearch(self):\n",
    "        cv = 10\n",
    "        clf = RandomizedSearchCV(self.model, self.hyperparameters,\n",
    "                                 random_state = 1, n_iter = 100,\n",
    "                                 cv = cv, verbose = 0, n_jobs = -1)\n",
    "        best_model = clf.fit(self.X_train, self.y_train)\n",
    "        message = (best_model.best_score_, best_model.best_params_)\n",
    "        print(\"Best : %f using %s\" % (message))\n",
    "        \n",
    "        return best_model, best_model.best_params_\n",
    "    \n",
    "    def BestModelPredict(self, X_test):\n",
    "        best_model, _ = self.RandomSearch()\n",
    "        pred = best_model.predict(X_test)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridSearch(object):\n",
    "    def __init__(self, X_train, y_train, model, hyperparameters):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.model = model\n",
    "        self.hyperparameters = hyperparameters\n",
    "    \n",
    "    def GridSearch(self):\n",
    "        cv = 10\n",
    "        clf = GridSearchCV(self.model, self.hyperparameters,\n",
    "                          cv = cv, verbose = 0, n_jobs = -1)\n",
    "        best_model = clf.fit(self.X_train, self.y_train)\n",
    "        message = (best_model.best_score_, best_model.best_params_)\n",
    "        print(\"Best : %f using %s\" %(message))\n",
    "        \n",
    "        return best_model, best_model.best_params_\n",
    "    \n",
    "    def BestModelPredict(self, X_test):\n",
    "        best_model, _ = self.GridSearch()\n",
    "        pred = best_model.predict(X_test)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty = ['l1', 'l2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = uniform(loc = 0, scale = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = dict(C=C, penalty = penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_RandSearch = RandomSearch(X_train_sc, y_train_sc, model, hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prediction_LR = LR_RandSearch.BestModelPredict(X_test_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def floatingDecimals(f_val, dec = 3):\n",
    "    prc = \"{:.\"+str(dec)+\"f}\"\n",
    "    \n",
    "    return float(prc.format(f_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Prediction on test set is :\", floatingDecimals((y_test_sc == Prediction_LR).mean(), 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
